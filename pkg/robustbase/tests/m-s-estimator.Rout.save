
R version 2.14.1 (2011-12-22)
Copyright (C) 2011 The R Foundation for Statistical Computing
ISBN 3-900051-07-0
Platform: x86_64-unknown-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> ## Test implementation of M-S estimator
> require(robustbase)
Loading required package: robustbase
> lmrob.conv.cc <- robustbase:::lmrob.conv.cc
> lmrob.psi2ipsi <- robustbase:::lmrob.psi2ipsi
> lmrob.wgtfun <- robustbase:::lmrob.wgtfun
> 
> ## dataset with factors and continuous variables:
> data(education)
> education <- within(education, Region <- factor(Region))
> ## for testing purposes:
> education2 <- within(education, Group <- factor(rep(1:3, length.out=length(Region))))
>                      
> ## Test lmrob.split (type fii is the only problematic type)
> testFun <- function(formula, x1.idx) {
+     obj <- lm(formula, education2)
+     mf <- obj$model
+     ret <- lmrob.split(mf, type="fii")
+     if (missing(x1.idx)) {
+         print(ret$x1.idx)
+         return(which(unname(ret$x1.idx)))
+     }
+     stopifnot(all.equal(x1.idx, which(unname(ret$x1.idx))))
+ }
> testFun(Y ~ 1, integer(0))
> testFun(Y ~ X1*X2*X3, integer(0))
> testFun(Y ~ Region + X1 + X2 + X3, 1:4)
> testFun(Y ~ 0 + Region + X1 + X2 + X3, 1:4)
> testFun(Y ~ Region*X1 + X2 + X3, c(1:5, 8:10))
> testFun(Y ~ Region*X1 + X2 + X3 + Region*Group, c(1:5, 8:18))
> testFun(Y ~ Region*X1 + X2 + X3 + Region*Group*X2, c(1:6, 8:29))
> testFun(Y ~ Region*X1 + X2 + Region*Group*X2, 1:28)
> testFun(Y ~ Region*X1 + X2 + Region:Group:X2, 1:21)
> testFun(Y ~ Region*X1 + X2*X3 + Region:Group:X2, c(1:6, 8:10, 12:23))
> testFun(Y ~ (X1+X2+X3+Region)^2, c(1:7,10:12,14:19))
> testFun(Y ~ (X1+X2+X3+Region)^3, c(1:19, 21:29))
> testFun(Y ~ (X1+X2+X3+Region)^4, 1:32)
> testFun(Y ~ Region:X1:X2 + X1*X2, c(1, 4:7))
> 
> ## Test subsampling algorithm
> m_s_subsample <- function(x1, x2, y, control, orthogonalize=TRUE) {
+     x1 <- as.matrix(x1)
+     x2 <- as.matrix(x2)
+     y <- y
+     storage.mode(x1) <- "double"
+     storage.mode(x2) <- "double"
+     storage.mode(y) <- "double"
+     
+     z <- .C(robustbase:::R_lmrob_M_S,
+             X1=x1,
+             X2=x2,
+             y=y,
+             res=double(length(y)),
+             n=length(y),
+             p1=ncol(x1),
+             p2=ncol(x2),
+             nResample=as.integer(control$nResample),
+             scale=double(1),
+             b1=double(ncol(x1)),
+             b2=double(ncol(x2)),
+             tuning_chi=as.double(control$tuning.chi),
+             ipsi=as.integer(lmrob.psi2ipsi(control$psi)),
+             bb=as.double(control$bb),
+             K_m_s=as.integer(control$k.m_s),
+             max_k=as.integer(control$k.max),
+             rel_tol=as.double(control$rel.tol),
+             converged=logical(1),
+             trace_lev=as.integer(control$trace.lev),
+             orthogonalize=as.logical(orthogonalize),
+             subsample=TRUE,
+             descent=FALSE)
+     z[c("b1", "b2", "scale")]
+ }
> 
> control <- lmrob.control()
> obj <- lm(Y ~ Region + X1 + X2 + X3, education)
> splt <- lmrob.split(obj$model)
> y <- education$Y
> 
> ## test orthogonalizing
> x1 <- splt$x1
> x2 <- splt$x2
> tmp <- lmrob.lar(x1, y, control)
> y.tilde <- tmp$resid
> t1 <- tmp$coef
> x2.tilde <- x2
> T2 <- matrix(0, nrow=ncol(x1), ncol=ncol(x2))
> for (i in 1:ncol(x2)) {
+     tmp <- lmrob.lar(x1, x2[,i], control)
+     x2.tilde[,i] <- tmp$resid
+     T2[,i] <- tmp$coef
+ }
> set.seed(10)
> res1 <- m_s_subsample(x1, x2.tilde, y.tilde, control, FALSE)
> res1 <- within(res1, b1 <- drop(t1 + b1 - T2 %*% b2))
> set.seed(10)
> res2 <- m_s_subsample(x1, x2, y, control, TRUE)
> stopifnot(all.equal(res1, res2))
> 
> res <- list()
> set.seed(0)
> time <- system.time(for (i in 1:100) {
+     tmp <- m_s_subsample(x1, x2.tilde, y.tilde, control, FALSE)
+     res[[i]] <- unlist(within(tmp, b1 <- drop(t1 + b1 - T2 %*% b2)))
+ })
> cat('Time elapsed in subsampling: ', time,'\n')
Time elapsed in subsampling:  0.385 0 0.388 0 0 
> ## show a summary of the results
> res1 <- do.call(rbind, res)
> summary(res1[,1:8])
      b11               b12              b13               b14       
 Min.   :-396.39   Min.   :-39.07   Min.   :-17.236   Min.   :16.22  
 1st Qu.:-224.09   1st Qu.:-23.38   1st Qu.: -8.241   1st Qu.:30.35  
 Median :-165.41   Median :-21.43   Median : -7.385   Median :32.55  
 Mean   :-166.64   Mean   :-21.52   Mean   : -7.404   Mean   :32.58  
 3rd Qu.:-102.66   3rd Qu.:-18.39   3rd Qu.: -6.478   3rd Qu.:35.95  
 Max.   : -33.62   Max.   :-12.23   Max.   :  1.154   Max.   :42.25  
      b21                b22               b23             scale      
 Min.   :-0.03808   Min.   :0.03168   Min.   :0.2953   Min.   :29.79  
 1st Qu.: 0.02009   1st Qu.:0.04004   1st Qu.:0.5024   1st Qu.:30.30  
 Median : 0.03095   Median :0.04672   Median :0.6382   Median :30.82  
 Mean   : 0.03050   Mean   :0.04594   Mean   :0.6346   Mean   :30.84  
 3rd Qu.: 0.04718   3rd Qu.:0.05110   3rd Qu.:0.8056   3rd Qu.:31.29  
 Max.   : 0.09728   Max.   :0.06792   Max.   :1.1109   Max.   :32.12  
> ## compare with fast S solution
> obj <- lmrob(Y ~ Region + X1 + X2 + X3, education, init="S")
> coef(obj)
  (Intercept)       Region2       Region3       Region4            X1 
-135.72600303  -20.64572279   -9.84882085   24.58011727    0.03405595 
           X2            X3 
   0.04327562    0.57895757 
> obj$scale
[1] 26.40388
> 
> ## Test descent algorithm
> m_s_descent <- function(x1, x2, y, control, b1, b2, scale) {
+     x1 <- as.matrix(x1)
+     x2 <- as.matrix(x2)
+     y <- y
+     storage.mode(x1) <- "double"
+     storage.mode(x2) <- "double"
+     storage.mode(y) <- "double"
+     
+     z <- .C(robustbase:::R_lmrob_M_S,
+             X1=x1,
+             X2=x2,
+             y=y,
+             res=double(length(y)),
+             n=length(y),
+             p1=ncol(x1),
+             p2=ncol(x2),
+             nResample=as.integer(control$nResample),
+             scale=as.double(scale),
+             b1=as.double(b1),
+             b2=as.double(b2),
+             tuning_chi=as.double(control$tuning.chi),
+             ipsi=as.integer(lmrob.psi2ipsi(control$psi)),
+             bb=as.double(control$bb),
+             K_m_s=as.integer(control$k.m_s),
+             max_k=as.integer(control$k.max),
+             rel_tol=as.double(control$rel.tol),
+             converged=logical(1),
+             trace_lev=as.integer(control$trace.lev),
+             orthogonalize=FALSE,
+             subsample=FALSE,
+             descent=TRUE)
+     z[c("b1", "b2", "scale", "res")]
+ }
> 
> find_scale <- function(r, s0, n, p, control) {
+     c.chi <- lmrob.conv.cc(control$psi, control$tuning.chi)
+     
+     b <- .C(robustbase:::R_lmrob_S,
+             x = double(1),
+             y = as.double(r),
+             n = as.integer(n),
+             p = as.integer(p),
+             nResample = 0L,
+             scale = as.double(s0),
+             coefficients = double(p),
+             as.double(c.chi),
+             as.integer(lmrob.psi2ipsi(control$psi)),
+             as.double(control$bb),
+             best_r = 0L,
+             groups = 0L,
+             n.group = 0L,
+             k.fast.s = 0L,
+             k.iter = 0L,
+             refine.tol = as.double(control$refine.tol),
+             converged = logical(1),
+             trace.lev = 0L
+             )[c("coefficients", "scale", "k.iter", "converged")]
+     b$scale
+ }
> 
> ## what should it be:
> m_s_descent_Ronly<- function(x1, x2, y, control, b1, b2, scale) {
+     n <- length(y)
+     p1 <- ncol(x1)
+     p2 <- ncol(x2)
+     p <- p1+p2
+     t2 <- b2
+     t1 <- b1
+     rs <- drop(y - x1 %*% b1 - x2 %*% b2)
+     sc <- scale
+     ## do refinement steps
+     ## do maximally control$k.max iterations
+     ## stop if converged
+     ## stop after k.fast.m_s step of no improvement
+     if (control$trace.lev > 4) cat("scale:", scale, "\n")
+     if (control$trace.lev > 4) cat("res:", rs, "\n")
+     nnoimprovement <- nref <- 0; conv <- FALSE
+     while((nref <- nref + 1) <= control$k.max && !conv &&
+           nnoimprovement < control$k.m_s) {
+         ## STEP 1: UPDATE B2
+         y.tilde <- y - x1 %*% t1
+         w <- lmrob.wgtfun(rs / sc, control$tuning.chi, control$psi)
+         if (control$trace.lev > 4) cat("w:", w, "\n")
+         z2 <- lm.wfit(x2, y.tilde, w)
+         t2 <- z2$coef
+         if (control$trace.lev > 4) cat("t2:", t2, "\n")
+         rs <- y - x2 %*% t2
+         ## STEP 2: OBTAIN M-ESTIMATE OF B1
+         z1 <- lmrob.lar(x1, rs, control)
+         t1 <- z1$coef
+         if (control$trace.lev > 4) cat("t1:", t1, "\n")
+         rs <- z1$resid
+         ## STEP 3: COMPUTE THE SCALE ESTIMATE
+         sc <- find_scale(rs, sc, n, p, control)
+         if (control$trace.lev > 4) cat("sc:", sc, "\n")
+         ## STEP 4: CHECK FOR CONVERGENCE
+         #...
+         ## STEP 5: UPDATE BEST FIT
+         if (sc < scale) {
+             scale <- sc
+             b1 <- t1
+             b2 <- t2
+             nnoimprovement <- 0
+         } else nnoimprovement <- nnoimprovement + 1
+     }
+     ## STEP 6: FINISH
+     if (nref == control$k.max)
+         warning("M-S estimate: maximum number of refinement steps reached.")
+     
+     list(b1=b1, b2=b2, scale=scale, res=rs)
+ }
> 
> control2 <- control
> #control2$trace.lev <- 5
> control2$k.max <- 1
> stopifnot(all.equal(m_s_descent(x1, x2, y, control2, res2$b1, res2$b2, res2$scale+10),
+                     m_s_descent_Ronly(x1, x2, y, control2, res2$b1, res2$b2, res2$scale+10),
+                     check.attr=FALSE))
> 
> ## control$k.m_s <- 100
> res3 <- list()
> time <- system.time(for (i in 1:100) {
+     res3[[i]] <- unlist(m_s_descent(x1, x2, y, control, res[[i]][1:4], res[[i]][5:7], res[[i]][8]))
+ })
> cat('Time elapsed in descent proc: ', time,'\n')
Time elapsed in descent proc:  0.08 0 0.081 0 0 
> 
> ## show a summary of the results
> res4 <- do.call(rbind, res3)
> summary(res4[,1:8])
      b11               b12              b13               b14       
 Min.   :-396.39   Min.   :-39.07   Min.   :-16.100   Min.   :16.22  
 1st Qu.:-221.93   1st Qu.:-22.83   1st Qu.: -8.680   1st Qu.:27.92  
 Median :-161.54   Median :-21.28   Median : -7.837   Median :30.86  
 Mean   :-164.48   Mean   :-20.66   Mean   : -8.121   Mean   :31.00  
 3rd Qu.:-101.67   3rd Qu.:-18.09   3rd Qu.: -6.951   3rd Qu.:33.15  
 Max.   : -28.78   Max.   :-12.24   Max.   : -4.032   Max.   :42.25  
      b21                b22               b23             scale      
 Min.   :-0.02141   Min.   :0.03098   Min.   :0.3260   Min.   :29.79  
 1st Qu.: 0.02153   1st Qu.:0.03933   1st Qu.:0.5024   1st Qu.:30.30  
 Median : 0.04100   Median :0.04272   Median :0.6376   Median :30.48  
 Mean   : 0.04017   Mean   :0.04411   Mean   :0.6356   Mean   :30.70  
 3rd Qu.: 0.05842   3rd Qu.:0.04794   3rd Qu.:0.7799   3rd Qu.:30.96  
 Max.   : 0.08613   Max.   :0.06473   Max.   :1.1109   Max.   :32.11  
> 
> plot(res1[, "scale"], res4[,"scale"])
> 
> ## Test lmrob.M.S
> x <- model.matrix(obj)
> control$trace.lev <- 3
> set.seed(1001)
> obj2 <- lmrob.M.S(x, y, control, obj$model)
starting with subsampling procedure...
Step 0: new candidate with sc = 104.088811486156
Step 5: new candidate with sc = 68.4977242622213
Step 7: new candidate with sc = 37.448652862062
Step 12: new candidate with sc = 35.1596230289069
Step 20: new candidate with sc = 31.9348857445637
Step 355: new candidate with sc = 31.5550607183877
Finished M-S subsampling with scale = 31.5550607183877
b1: -19.158290 10.753696 11.130900 25.192340 
b2: -0.029633 0.051415 0.809801 
starting with descent procedure...
Refinement step 3: better fit, scale: 31.5205071818621
Refinement step 4: better fit, scale: 30.9466521282569
Refinement step 5: better fit, scale: 30.8180005418827
Refinement step 6: better fit, scale: 30.758316670988
Refinement step 7: better fit, scale: 30.6979460767994
Refinement step 8: better fit, scale: 30.6043365897621
Refinement step 9: better fit, scale: 30.588042369233
descent procedure: not converged.
b1: -197.193523 -21.179008 -7.561589 27.098607 
b2: 0.051545 0.045065 0.702375 
> resid <- drop(y - x %*% obj2$coef)
> stopifnot(all.equal(resid, obj2$resid, check.attr=FALSE))
> 
> ## Test direct call to lmrob
> set.seed(13)
> obj1 <- lmrob(Y ~ Region + X1 + X2 + X3, education)
> summary(obj1)

Call:
lmrob(formula = Y ~ Region + X1 + X2 + X3, data = education)

Weighted Residuals:
    Min      1Q  Median      3Q     Max 
-63.074 -15.516  -1.571  23.369 174.769 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)   
(Intercept) -150.54998  143.26726  -1.051  0.29921   
Region2      -12.49679   16.64842  -0.751  0.45696   
Region3      -10.67307   15.95032  -0.669  0.50698   
Region4       21.85618   16.98728   1.287  0.20511   
X1             0.04170    0.05048   0.826  0.41331   
X2             0.04337    0.01374   3.155  0.00292 **
X3             0.61219    0.35201   1.739  0.08917 . 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Robust residual standard error: 31.06 
Convergence in 19 IRWLS iterations

Robustness weights: 
 observation 50 is an outlier with |weight| = 0 ( < 0.002); 
 7 weights are ~= 1. The remaining 42 ones are summarized as
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.2995  0.8907  0.9518  0.8906  0.9867  0.9986 
Algorithmic parameters: 
tuning.chi         bb tuning.psi refine.tol    rel.tol 
 1.5476400  0.5000000  4.6850610  0.0000001  0.0000001 
 nResample     max.it     groups    n.group   best.r.s   k.fast.s      k.max 
       500         50          5        400          2          1        200 
     k.m_s  trace.lev compute.rd  numpoints 
        20          0          0         10 
       psi     method        cov split.type 
"bisquare"     "M-SM"  ".vcov.w"        "f" 
seed : int(0) 
> out1 <- capture.output(summary(obj1))
> 
> set.seed(13)
> obj2 <- lmrob(Y ~ Region + X1 + X2 + X3, education, init="M-S")
> out2 <- capture.output(summary(obj2))
> 
> set.seed(13)
> obj3 <- lmrob(Y ~ Region + X1 + X2 + X3, education, init=lmrob.M.S)
> out3 <- capture.output(summary(obj3))
> 
> stopifnot(all.equal(out1[-(1:3)], out2[-(1:4)]),
+           all.equal(out1[-(1:3)], out3[-(1:4)]))
> 

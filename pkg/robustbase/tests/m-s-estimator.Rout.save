
R version 2.14.1 (2011-12-22)
Copyright (C) 2011 The R Foundation for Statistical Computing
ISBN 3-900051-07-0
Platform: x86_64-unknown-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> ## Test implementation of M-S estimator
> require(robustbase)
Loading required package: robustbase
> lmrob.conv.cc <- robustbase:::lmrob.conv.cc
> lmrob.psi2ipsi <- robustbase:::lmrob.psi2ipsi
> lmrob.wgtfun <- robustbase:::lmrob.wgtfun
> 
> ## dataset with factors and continuous variables:
> data(education)
> education <- within(education, Region <- factor(Region))
> ## for testing purposes:
> education2 <- within(education, Group <- factor(rep(1:3, length.out=length(Region))))
>                      
> ## Test lmrob.split
> testFun <- function(formula, x1.idx) {
+     obj <- lm(formula, education2)
+     mf <- obj$model
+     ret <- lmrob.split(mf)
+     if (missing(x1.idx)) {
+         print(ret$x1.idx)
+         return(which(unname(ret$x1.idx)))
+     }
+     stopifnot(all.equal(x1.idx, which(unname(ret$x1.idx))))
+ }
> testFun(Y ~ 1, integer(0))
> testFun(Y ~ X1*X2*X3, integer(0))
> testFun(Y ~ Region + X1 + X2 + X3, 1:4)
> testFun(Y ~ 0 + Region + X1 + X2 + X3, 1:4)
> testFun(Y ~ Region*X1 + X2 + X3, c(1:5, 8:10))
> testFun(Y ~ Region*X1 + X2 + X3 + Region*Group, c(1:5, 8:18))
> testFun(Y ~ Region*X1 + X2 + X3 + Region*Group*X2, c(1:6, 8:29))
> testFun(Y ~ Region*X1 + X2 + Region*Group*X2, 1:28)
> testFun(Y ~ Region*X1 + X2 + Region:Group:X2, 1:21)
> testFun(Y ~ Region*X1 + X2*X3 + Region:Group:X2, c(1:6, 8:10, 12:23))
> testFun(Y ~ (X1+X2+X3+Region)^2, c(1:7,10:12,14:19))
> testFun(Y ~ (X1+X2+X3+Region)^3, c(1:19, 21:29))
> testFun(Y ~ (X1+X2+X3+Region)^4, 1:32)
> 
> ## Test subsampling algorithm
> m_s_subsample <- function(x1, x2, y, control, orthogonalize=TRUE) {
+     x1 <- as.matrix(x1)
+     x2 <- as.matrix(x2)
+     y <- y
+     storage.mode(x1) <- "double"
+     storage.mode(x2) <- "double"
+     storage.mode(y) <- "double"
+     
+     z <- .C(robustbase:::R_lmrob_M_S,
+             X1=x1,
+             X2=x2,
+             y=y,
+             n=length(y),
+             p1=ncol(x1),
+             p2=ncol(x2),
+             nResample=as.integer(control$nResample),
+             scale=double(1),
+             b1=double(ncol(x1)),
+             b2=double(ncol(x2)),
+             tuning_chi=as.double(control$tuning.chi),
+             ipsi=as.integer(lmrob.psi2ipsi(control$psi)),
+             bb=as.double(control$bb),
+             K_m_s=as.integer(control$k.m_s),
+             max_k=as.integer(control$k.max),
+             rel_tol=as.double(control$rel.tol),
+             converged=logical(1),
+             trace_lev=as.integer(control$trace.lev),
+             orthogonalize=as.logical(orthogonalize),
+             subsample=TRUE,
+             descent=FALSE,
+             reweight=FALSE)
+     z[c("b1", "b2", "scale")]
+ }
> 
> control <- lmrob.control()
> obj <- lm(Y ~ Region + X1 + X2 + X3, education)
> splt <- lmrob.split(obj$model)
> y <- education$Y
> 
> ## test orthogonalizing
> x1 <- splt$x1
> x2 <- splt$x2
> tmp <- lmrob.lar(x1, y, control$rel.tol)
> y.tilde <- tmp$resid
> t1 <- tmp$coef
> x2.tilde <- x2
> T2 <- matrix(0, nrow=ncol(x1), ncol=ncol(x2))
> for (i in 1:ncol(x2)) {
+     tmp <- lmrob.lar(x1, x2[,i], control$rel.tol)
+     x2.tilde[,i] <- tmp$resid
+     T2[,i] <- tmp$coef
+ }
> set.seed(10)
> res1 <- m_s_subsample(x1, x2.tilde, y.tilde, control, FALSE)
> res1 <- within(res1, b1 <- drop(t1 + b1 - T2 %*% b2))
> set.seed(10)
> res2 <- m_s_subsample(x1, x2, y, control, TRUE)
> stopifnot(all.equal(res1, res2))
> 
> res <- list()
> set.seed(0)
> time <- system.time(for (i in 1:100) {
+     tmp <- m_s_subsample(x1, x2.tilde, y.tilde, control, FALSE)
+     res[[i]] <- unlist(within(tmp, b1 <- drop(t1 + b1 - T2 %*% b2)))
+ })
> cat('Time elapsed in subsampling: ', time,'\n')
Time elapsed in subsampling:  0.337 0.002 0.34 0 0 
> ## show a summary of the results
> summary(do.call(rbind, res))
      b11               b12              b13               b14       
 Min.   :-318.21   Min.   :-32.84   Min.   :-19.386   Min.   :19.97  
 1st Qu.:-220.48   1st Qu.:-23.36   1st Qu.: -8.376   1st Qu.:30.22  
 Median :-165.13   Median :-21.37   Median : -7.415   Median :32.64  
 Mean   :-160.86   Mean   :-21.20   Mean   : -7.436   Mean   :32.86  
 3rd Qu.: -98.35   3rd Qu.:-18.33   3rd Qu.: -6.331   3rd Qu.:36.26  
 Max.   : -33.62   Max.   :-12.23   Max.   :  1.154   Max.   :42.25  
      b21                b22               b23             scale      
 Min.   :-0.03808   Min.   :0.03168   Min.   :0.2953   Min.   :29.79  
 1st Qu.: 0.01643   1st Qu.:0.03996   1st Qu.:0.4928   1st Qu.:30.37  
 Median : 0.02947   Median :0.04682   Median :0.6270   Median :30.82  
 Mean   : 0.02987   Mean   :0.04567   Mean   :0.6217   Mean   :30.84  
 3rd Qu.: 0.04804   3rd Qu.:0.05110   3rd Qu.:0.7847   3rd Qu.:31.33  
 Max.   : 0.09728   Max.   :0.06792   Max.   :0.9480   Max.   :32.12  
> ## compare with fast S solution
> obj <- lmrob(Y ~ Region + X1 + X2 + X3, education, init="S")
> coef(obj)
  (Intercept)       Region2       Region3       Region4            X1 
-135.72600303  -20.64572279   -9.84882085   24.58011727    0.03405595 
           X2            X3 
   0.04327562    0.57895757 
> obj$scale
[1] 26.40388
> 
> ## Test descent algorithm
> m_s_descent <- function(x1, x2, y, control, b1, b2, scale) {
+     x1 <- as.matrix(x1)
+     x2 <- as.matrix(x2)
+     y <- y
+     storage.mode(x1) <- "double"
+     storage.mode(x2) <- "double"
+     storage.mode(y) <- "double"
+     
+     z <- .C(robustbase:::R_lmrob_M_S,
+             X1=x1,
+             X2=x2,
+             y=y,
+             n=length(y),
+             p1=ncol(x1),
+             p2=ncol(x2),
+             nResample=as.integer(control$nResample),
+             scale=as.double(scale),
+             b1=as.double(b1),
+             b2=as.double(b2),
+             tuning_chi=as.double(control$tuning.chi),
+             ipsi=as.integer(lmrob.psi2ipsi(control$psi)),
+             bb=as.double(control$bb),
+             K_m_s=as.integer(control$k.m_s),
+             max_k=as.integer(control$k.max),
+             rel_tol=as.double(control$rel.tol),
+             converged=logical(1),
+             trace_lev=as.integer(control$trace.lev),
+             orthogonalize=FALSE,
+             subsample=FALSE,
+             descent=TRUE,
+             reweight=FALSE)
+     z[c("b1", "b2", "scale")]
+ }
> 
> find_scale <- function(r, s0, n, p, control) {
+     c.chi <- lmrob.conv.cc(control$psi, control$tuning.chi)
+     
+     b <- .C(robustbase:::R_lmrob_S,
+             x = double(1),
+             y = as.double(r),
+             n = as.integer(n),
+             p = as.integer(p),
+             nResample = 0L,
+             scale = as.double(s0),
+             coefficients = double(p),
+             as.double(c.chi),
+             as.integer(lmrob.psi2ipsi(control$psi)),
+             as.double(control$bb),
+             best_r = 0L,
+             groups = 0L,
+             n.group = 0L,
+             k.fast.s = 0L,
+             k.iter = 0L,
+             refine.tol = as.double(control$refine.tol),
+             converged = logical(1),
+             trace.lev = 0L
+             )[c("coefficients", "scale", "k.iter", "converged")]
+     b$scale
+ }
> 
> ## what should it be:
> m_s_descent_Ronly<- function(x1, x2, y, control, b1, b2, scale) {
+     n <- length(y)
+     p1 <- ncol(x1)
+     p2 <- ncol(x2)
+     p <- p1+p2
+     t2 <- b2
+     t1 <- b1
+     rs <- drop(y - x1 %*% b1 - x2 %*% b2)
+     sc <- scale
+     ## do refinement steps
+     ## do maximally control$k.max iterations
+     ## stop if converged
+     ## stop after k.fast.m_s step of no improvement
+     cat("scale:", scale, "\n")
+     cat("res:", rs, "\n")
+     nnoimprovement <- nref <- 0; conv <- FALSE
+     while((nref <- nref + 1) <= control$k.max && !conv &&
+           nnoimprovement < control$k.m_s) {
+         ## STEP 1: UPDATE B2
+         y.tilde <- y - x1 %*% t1
+         w <- lmrob.wgtfun(rs / sc, control$tuning.chi, control$psi)
+         if (control$trace.lev > 4) cat("w:", w, "\n")
+         z2 <- lm.wfit(x2, y.tilde, w)
+         t2 <- z2$coef
+         if (control$trace.lev > 4) cat("t2:", t2, "\n")
+         rs <- y - x2 %*% t2
+         ## STEP 2: OBTAIN M-ESTIMATE OF B1
+         z1 <- lmrob.lar(x1, rs, control$rel.tol)
+         t1 <- z1$coef
+         if (control$trace.lev > 4) cat("t1:", t1, "\n")
+         rs <- z1$resid
+         ## STEP 3: COMPUTE THE SCALE ESTIMATE
+         sc <- find_scale(rs, sc, n, p, control)
+         if (control$trace.lev > 4) cat("sc:", sc, "\n")
+         ## STEP 4: CHECK FOR CONVERGENCE
+         #...
+         ## STEP 5: UPDATE BEST FIT
+         if (sc < scale) {
+             scale <- sc
+             b1 <- t1
+             b2 <- t2
+             nnoimprovement <- 0
+         } else nnoimprovement <- nnoimprovement + 1
+     }
+     ## STEP 6: FINISH
+     if (nref == control$k.max)
+         warning("M-S estimate: maximum number of refinement steps reached.")
+     
+     list(b1=b1, b2=b2, scale=scale)
+ }
> 
> control2 <- control
> #control2$trace.lev <- 5
> control2$k.max <- 1
> stopifnot(all.equal(m_s_descent(x1, x2, y, control2, res2$b1, res2$b2, res2$scale+10),
+                     m_s_descent_Ronly(x1, x2, y, control2, res2$b1, res2$b2, res2$scale+10),
+                     check.attr=FALSE))
scale: 41.39674 
res: -8.6527 -39.61602 17.43366 -22.81718 38.95789 0 85.94543 -25.4 32.23061 -42.50927 -5.684342e-14 14.25497 87.2957 91.53674 113.0937 -24.98671 -8.524785 -19.01129 -10.55104 13.87197 81.88584 38.56475 38.29496 -1.278185 -7.912545 3.98423 -6.470067 -4.663523 6.210023 -18.8613 -14.29412 -18.65335 -17.96482 2.064944 0 3.98423 16.62335 -6.678307 -41.71973 0 -21.63584 22.84989 21.78031 4.464618 -61.68664 -5.406487 15.93799 1.385498 -31.44817 149.1588 
> 
> 
> 
> ## time <- system.time(for (i in 1:100) {
> ##     res[[i]] <- unlist(m_s_descent(x1, x2, y, control, res[[i]][1:4], res[[i]][5:7], res[[i]][8]))
> ## })
> ## cat('Time elapsed in descent proc: ', time,'\n')
> 
